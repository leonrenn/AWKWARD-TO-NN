{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Awkward Arrays to Using NN\n",
    "---\n",
    "This notebook should help in order to process data from an awkward array to a format that can be used by the NN.<br>\n",
    "Awkward arrays are a data structure that is used in the HEP community. It is a nested array structure that can be used to store data in a more efficient way than a numpy array. <br>\n",
    "The data structure is described in more detail [here](https://awkward-array.org/doc/main/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awkward as ak\n",
    "from typing import List, Dict, Tuple\n",
    "import pandas as pd\n",
    "import uproot as ur\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Sample Data\n",
    "As it is shown for the example data, the length of the array is not the same for each event for _el_pt_.<br>\n",
    "This is a typical case in HEP data, where the number of particles in an event is not the same for each event.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# awkward array with event index, el_pt, met, event weight\n",
    "ak_array = ak.Array([[{\"Event\": 0, \"el_pt\": [10, 20, 3], \"met\": 20, \"eventWeight\": 1}, \n",
    "                      {\"Event\": 1, \"el_pt\": [], \"met\": 30, \"eventWeight\": 1}, \n",
    "                      {\"Event\": 2, \"el_pt\": [3], \"met\": 40, \"eventWeight\": 1}]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Process Awkward Arrays to Pandas Dataframe\n",
    "\n",
    "The function ***from_ak_2_df*** takes an awkward array as input and returns a pandas dataframe.<br>\n",
    "All the entries can be normalized when setting normalize to True.<br>\n",
    "Unimportant fields can be dropped by setting remove fields to the list of names.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_ak_2_df(ak_array: ak.Array,\n",
    "                 remove_fields: List[str],\n",
    "                 normalize: bool) -> Tuple[pd.DataFrame,\n",
    "                                            Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Convert awkward array to pandas dataframe.\n",
    "\n",
    "    1. Convert awkward array to pandas dataframe.\n",
    "    2. Remove fields.\n",
    "    3. Itertate over all fields.\n",
    "        1. Include an extra column for the length of the list.\n",
    "        2. Get the maximum length of the list.\n",
    "        3. Pad list to the same length.\n",
    "        4. Convert to list after filling nones with 0.\n",
    "    4. Normalize data.\n",
    "        1. Iterate over all columns.\n",
    "            1. Check if column is a list.\n",
    "                1. Flatten the ak array.\n",
    "                2. Assign minimum and maximum values.\n",
    "                3. Normalization factor in dictionary.\n",
    "            1. Get minimum and maximum values of column.\n",
    "            2. Normalize column.\n",
    "    5. Return dataframe and normalized dict with dropped index.\n",
    "\n",
    "    Parameters:\n",
    "        ak_array (ak.Array): Awkward array.\n",
    "        remove_fields (List[str]): List of fields to remove.\n",
    "        normalize (bool): Normalize data.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, Dict[str, float]]: Tuple of dataframe and normalization factors.\n",
    "    \"\"\"\n",
    "    # 0. Normalization factors\n",
    "    if normalize is True:\n",
    "        norm_factors: Dict[str, float] = {}\n",
    "    else:\n",
    "        norm_factors = None\n",
    "    # 1. Convert awkward array to pandas dataframe.\n",
    "    df = pd.DataFrame(ak_array.tolist()[0])\n",
    "    # 2. Remove fields.\n",
    "    df = df.drop(remove_fields, axis=1)\n",
    "\n",
    "    # 3. Itertate over all fields.\n",
    "    for field in df.columns:\n",
    "        if type(df[field][0]) == list:\n",
    "            # 1. Include an extra column for the length of the list.\n",
    "            df[field + \"_len\"] = df[field].apply(lambda x: len(x))\n",
    "            # 2. Get the maximum length of the list.\n",
    "            max_len = df[field + \"_len\"].max()\n",
    "            # 3. Pad list to the same length.\n",
    "            none_array = ak.pad_none(ak_array[field][0], \n",
    "                                     target=max_len,\n",
    "                                     clip=True)\n",
    "            # 4. Convert to list after filling nones with 0.\n",
    "            df[field] = ak.fill_none(none_array, 0).tolist()\n",
    "        \n",
    "    # 4. Normalize data.\n",
    "    if normalize is True:\n",
    "        # 1. Iterate over all columns.\n",
    "        for field in df.columns:\n",
    "            # 1. Check if column is a list.\n",
    "            if type(df[field][0]) == list:\n",
    "                # 1. Flatten the ak array.\n",
    "                flat_array = ak.flatten(ak_array[field])\n",
    "                # 2. Assign minimum and maximum values.\n",
    "                min_value = abs(ak.min(flat_array))\n",
    "                max_value = abs(ak.max(flat_array))\n",
    "                if min_value > max_value:\n",
    "                    norm_factors[field] = 1/min_value\n",
    "                else:\n",
    "                    norm_factors[field] = 1/max_value\n",
    "                # 3. Normalization factor in dictionary.\n",
    "                df[field] = df[field].apply(\n",
    "                    lambda x: [i * norm_factors[field] for i in x])\n",
    "            else:\n",
    "                # 1. Get minimum and maximum values of column.\n",
    "                min_value = abs(df[field].min())\n",
    "                max_value = abs(df[field].max())\n",
    "                if min_value > max_value:\n",
    "                    norm_factors[field] = 1/min_value\n",
    "                else:\n",
    "                    norm_factors[field] = 1/max_value\n",
    "                # 2. Normalize column.\n",
    "                df[field] = df[field].apply(\n",
    "                    lambda x: x * norm_factors[field])\n",
    "    else:\n",
    "        pass\n",
    "    # 5. Return dataframe and normalized dict with dropped index.\n",
    "    return df.reset_index(drop=True), norm_factors\n",
    "\n",
    "df, norm_factor = from_ak_2_df(ak_array=ak_array, \n",
    "                               remove_fields=[\"Event\", \"eventWeight\"],\n",
    "                               normalize=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all Possible Signal Regions with Acceptance and Rejection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2964\n"
     ]
    }
   ],
   "source": [
    "def sr_acc_rej(path: str):\n",
    "    count=0\n",
    "    # 1. Check if directory exists.\n",
    "    if os.path.isdir(path) is False:\n",
    "        raise ValueError(\"Directory does not exist.\")\n",
    "    # 2. Get all files in directory.\n",
    "    files = os.listdir(path)\n",
    "    # 3. Iterate over all files.\n",
    "    for file in files:\n",
    "        # 1. Check if file is a root file.\n",
    "        if file.endswith(\".root\"):\n",
    "            # 1. Open root file.\n",
    "            root_file: ur.ReadOnlyFile = ur.open(path + file)\n",
    "            # 2. Get tree.\n",
    "            n = root_file[\"ntuple\"].keys()\n",
    "            count+=len(n)\n",
    "    print(count)\n",
    "            \n",
    "\n",
    "sr_acc_rej(path=\"data/100293/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
